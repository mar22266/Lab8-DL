{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66bd8aa0",
   "metadata": {},
   "source": [
    "# Laboratorio 8 Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40ccf5",
   "metadata": {},
   "source": [
    "## Task 2 - Teoría\n",
    "Responda claramente y con una extensión adecuada las siguientes preguntas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14f363",
   "metadata": {},
   "source": [
    "1. ¿Cuál es el problema del gradiente de fuga en las redes LSTM y cómo afecta la efectividad de LSTM para el\n",
    "pronóstico de series temporales?\n",
    "\n",
    "El gradiente de fuga ocurre cuando, durante el retropropagado a través del tiempo, los gradientes se vuelven exponencialmente pequeños a medida que se retrocede por muchos pasos temporales, impidiendo actualizar pesos en capas/tiempos lejanos. En RNN clásicas esto limita severamente el aprendizaje de dependencias de largo plazo, sesgando los modelos hacia patrones muy recientes y produciendo pronósticos que no capturan ciclos o efectos retardados extensos. Las LSTM se diseñaron justamente para mitigar este fenómeno mediante una ruta lineal de memoria y compuertas (entrada/olvido/salida) que regulan el flujo de información y gradientes, permitiendo mantener señales por más tiempo y entrenar secuencias largas con mayor estabilidad. Aun así, con secuencias extremadamente largas, mala inicialización o arquitectura inadecuada, puede reaparecer la atenuación o inestabilidad del entrenamiento, afectando la precisión y la calibración de los intervalos de pronóstico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2305f",
   "metadata": {},
   "source": [
    "\n",
    "2. ¿Cómo se aborda la estacionalidad en los datos de series temporales cuando se utilizan LSTM para realizar\n",
    "pronósticos y qué papel juega la diferenciación en el proceso?\n",
    "\n",
    "Para tratar la estacionalidad con LSTM se combinan: ingeniería de atributos estacionales, ventanas de entrada que cubran múltiplos del período estacional, y preprocesamiento. Aunque LSTM puede modelar datos no estacionarios, la diferenciación y la eliminación de tendencia suelen estabilizar la media/varianza y facilitan el aprendizaje, especialmente cuando la serie tiene estacionalidad fuerte y larga. En la práctica, elegir una ventana alineada con el período ayuda a que la red “vea” ciclos completos; por ejemplo, usar 24 pasos para datos horarios por su componente diario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ce1fd",
   "metadata": {},
   "source": [
    "3. ¿Cuál es el concepto de \"tamaño de ventana\" en el pronóstico de series temporales con LSTM y cómo afecta\n",
    "la elección del tamaño de ventana a la capacidad del modelo para capturar patrones a corto y largo plazo?\n",
    "\n",
    "El tamaño de ventana es la longitud de la secuencia histórica que se entrega al LSTM como entrada en cada ejemplo supervisado. Ventanas cortas priorizan señales de corto plazo, reducen dimensionalidad y aceleran el entrenamiento, pero pueden perder ciclos largos; ventanas largas permiten capturar tendencias y estacionalidades extensas, a costa de más parámetros, mayor dificultad de optimización y riesgo de ruido. La elección óptima depende del horizonte a predecir y de las periodicidades de la serie: estudios empíricos muestran que el rendimiento varía sistemáticamente con el tamaño de ventana (por ejemplo, una ventana de un día para predecir 1 hora adelante; 2–4 días para 3 horas), por lo que conviene validarlo como hiperparámetro y alinearlo con los períodos estacionales predominantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd1ce4",
   "metadata": {},
   "source": [
    "### Referencias:\n",
    "\n",
    "- Xu, Z., & Xu, Z. (2024, March 18). Prevent the Vanishing Gradient Problem with LSTM | Baeldung on Computer Science. Baeldung on Computer Science. https://www.baeldung.com/cs/lstm-vanishing-gradient-prevention\n",
    "- Keith, M. (2025, January 21). Exploring the LSTM Neural network model for Time Series. Towards Data Science. https://towardsdatascience.com/exploring-the-lstm-neural-network-model-for-time-series-8b7685aa8cf/\n",
    "- Sable, A. (2021, April 9). Time Series Forecasting with Regression and LSTM | Paperspace Blog. Paperspace by DigitalOcean Blog. https://blog.paperspace.com/time-series-forecasting-regression-and-lstm/\n",
    "- Ortiz, J. a. R. J. E. (n.d.). Skforecast: forecasting series temporales con Python, Machine Learning y Scikit-learn. https://cienciadedatos.net/documentos/py27-forecasting-series-temporales-python-scikitlearn\n",
    "- Baeldung. (2024, marzo 18). Prevent the vanishing gradient problem with LSTM. https://www.baeldung.com/cs/lstm-vanishing-gradient-prevention\n",
    "- Scartezini, C. (2023, octubre 30). Multi-variate time series with deep learning. Medium. https://medium.com/@carmenscartezini/multi-variate-time-series-with-deep-learning-40a8c89afa9e\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
