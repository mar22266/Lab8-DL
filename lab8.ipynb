{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6141cd0a",
   "metadata": {},
   "source": [
    "# Andre marroquin 22266\n",
    "# sergio orellana 221122\n",
    "# nelson garcia\n",
    "# joaquin puente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57888e27",
   "metadata": {},
   "source": [
    "---------\n",
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9811d5",
   "metadata": {},
   "source": [
    "## preparacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf247775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fechas train: 2013-01-01 00:00:00 -> 2017-12-31 00:00:00\n",
      "fechas test : 2018-01-01 00:00:00 -> 2018-03-31 00:00:00\n",
      "cols train: ['date', 'store', 'item', 'sales', 'dow', 'month', 'is_weekend', 'sin_dow', 'cos_dow', 'sin_month', 'cos_month', 'sales_log1p']\n",
      "cols test : ['id', 'date', 'store', 'item', 'dow', 'month', 'is_weekend', 'sin_dow', 'cos_dow', 'sin_month', 'cos_month']\n"
     ]
    }
   ],
   "source": [
    "# importacion de librerias\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# carga de datasets\n",
    "def load_datasets(train_path: str, test_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train = pd.read_csv(train_path)\n",
    "    test  = pd.read_csv(test_path)\n",
    "\n",
    "    train.columns = [c.strip().lower() for c in train.columns]\n",
    "    test.columns  = [c.strip().lower() for c in test.columns]\n",
    "\n",
    "    if 'date' in train.columns:\n",
    "        train['date'] = pd.to_datetime(train['date'], errors='coerce')\n",
    "    if 'date' in test.columns:\n",
    "        test['date'] = pd.to_datetime(test['date'], errors='coerce')\n",
    "\n",
    "    for c in ['store', 'item']:\n",
    "        if c in train.columns:\n",
    "            train[c] = pd.to_numeric(train[c], errors='coerce').fillna(0).astype(int)\n",
    "        if c in test.columns:\n",
    "            test[c]  = pd.to_numeric(test[c], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    if 'sales' in train.columns:\n",
    "        train['sales'] = pd.to_numeric(train['sales'], errors='coerce')\n",
    "\n",
    "    # ordenar sin reset_index \n",
    "    train = train.sort_values([col for col in ['store', 'item', 'date'] if col in train.columns])\n",
    "    test  = test.sort_values([col for col in ['store', 'item', 'date'] if col in test.columns])\n",
    "    return train, test\n",
    "\n",
    "# limppieza de datos\n",
    "# faltanetes en sales => 0.0 negativos => 0.0 winsorizacion por (store,item) en p1 y p99\n",
    "def clean_sales(train: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'sales' not in train.columns:\n",
    "        raise ValueError(\"no se encontro la columna 'sales' en train\")\n",
    "\n",
    "    df = train.copy()\n",
    "    df['sales'] = df['sales'].fillna(0.0).astype(float)\n",
    "    df.loc[df['sales'] < 0.0, 'sales'] = 0.0\n",
    "\n",
    "    if {'store', 'item'}.issubset(df.columns):\n",
    "        g = df.groupby(['store', 'item'])['sales']\n",
    "        q01 = g.transform(lambda s: s.quantile(0.01) if s.notna().any() else np.nan)\n",
    "        q99 = g.transform(lambda s: s.quantile(0.99) if s.notna().any() else np.nan)\n",
    "\n",
    "        sales_vals = df['sales'].to_numpy()\n",
    "        q01_vals = q01.to_numpy()\n",
    "        q99_vals = q99.to_numpy()\n",
    "\n",
    "        mask = ~np.isnan(q01_vals)\n",
    "        sales_vals[mask] = np.maximum(sales_vals[mask], q01_vals[mask])\n",
    "        mask = ~np.isnan(q99_vals)\n",
    "        sales_vals[mask] = np.minimum(sales_vals[mask], q99_vals[mask])\n",
    "\n",
    "        df = df.copy()\n",
    "        df.loc[:, 'sales'] = sales_vals\n",
    "    else:\n",
    "        q1  = df['sales'].quantile(0.01)\n",
    "        q99 = df['sales'].quantile(0.99)\n",
    "        df['sales'] = df['sales'].clip(lower=q1, upper=q99)\n",
    "\n",
    "    return df\n",
    "\n",
    "# dict de listas para insertar columna a columna\n",
    "def _build_df_from_dict_of_lists(base: pd.DataFrame, extra: dict) -> pd.DataFrame:\n",
    "    data = {col: base[col].tolist() for col in base.columns}\n",
    "    for k, v in extra.items():\n",
    "        data[k] = list(v) if not isinstance(v, list) else v\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# agregar caracteristicas de calendario\n",
    "def add_calendar_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'date' not in df.columns:\n",
    "        raise ValueError(\"no se encontro la columna 'date'\")\n",
    "    dow = df['date'].dt.dayofweek.astype(int).tolist()  \n",
    "    month = df['date'].dt.month.astype(int).tolist()    \n",
    "    is_weekend = [1 if d >= 5 else 0 for d in dow]\n",
    "\n",
    "    sin_dow   = [math.sin(2 * math.pi * d / 7.0) for d in dow]\n",
    "    cos_dow   = [math.cos(2 * math.pi * d / 7.0) for d in dow]\n",
    "    sin_month = [math.sin(2 * math.pi * (m - 1) / 12.0) for m in month]\n",
    "    cos_month = [math.cos(2 * math.pi * (m - 1) / 12.0) for m in month]\n",
    "\n",
    "    extras = {\n",
    "        'dow': dow,\n",
    "        'month': month,\n",
    "        'is_weekend': is_weekend,\n",
    "        'sin_dow': sin_dow,\n",
    "        'cos_dow': cos_dow,\n",
    "        'sin_month': sin_month,\n",
    "        'cos_month': cos_month,\n",
    "    }\n",
    "    return _build_df_from_dict_of_lists(df, extras)\n",
    "\n",
    "# transformacion log1p del target sales\n",
    "def add_transformed_target(train: pd.DataFrame) -> pd.DataFrame:\n",
    "    if 'sales' not in train.columns:\n",
    "        raise ValueError(\"no se encontro la columna 'sales'\")\n",
    "    sales = train['sales'].astype(float).clip(lower=0.0)\n",
    "    sales_log1p = np.log1p(sales.to_numpy())\n",
    "    return _build_df_from_dict_of_lists(train, {'sales_log1p': sales_log1p})\n",
    "\n",
    "train_path = \"train.csv\" \n",
    "test_path  = \"test.csv\"  \n",
    "\n",
    "train_raw, test_raw = load_datasets(train_path, test_path)\n",
    "train_clean = clean_sales(train_raw)\n",
    "train_feat  = add_calendar_features(train_clean)\n",
    "train_feat  = add_transformed_target(train_feat)\n",
    "test_feat   = add_calendar_features(test_raw)\n",
    "\n",
    "# imprimir informacion basica\n",
    "print(\"fechas train:\", train_feat['date'].min(), \"->\", train_feat['date'].max())\n",
    "print(\"fechas test :\", test_feat['date'].min(),  \"->\", test_feat['date'].max())\n",
    "print(\"cols train:\", list(train_feat.columns))\n",
    "print(\"cols test :\", list(test_feat.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf76a6b",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54148c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tr: (109500, 120, 8) y_tr: (109500, 90) | x_va: (6000, 120, 8) y_va: (6000, 90)\n",
      "input shape (timesteps, features): (120, 8)\n"
     ]
    }
   ],
   "source": [
    "# procesamiento de datos split temporal y secuencias rapidas con stride semanal\n",
    "# horizonte de 3 meses \n",
    "horizon = 90\n",
    "# ventana de historia 120 dias\n",
    "window_size = 120\n",
    "\n",
    "# split temporal\n",
    "def compute_time_split(train_df: pd.DataFrame, horizon: int) -> Dict[str, pd.Timestamp]:\n",
    "    max_date = train_df['date'].max()\n",
    "    val_start = max_date - pd.Timedelta(days=horizon) + pd.Timedelta(days=1)\n",
    "    return {'max_date': max_date, 'val_start': val_start}\n",
    "\n",
    "# secuencias rapidas con stride semanal\n",
    "def series_to_tensor_fast(\n",
    "    df: pd.DataFrame,\n",
    "    window_size: int,\n",
    "    horizon: int,\n",
    "    val_start: pd.Timestamp,\n",
    "    stride_train: int = 7,\n",
    "    stride_val: int = 7\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # genera ventanas con stride\n",
    "    feats = ['sales_log1p', 'sin_dow', 'cos_dow', 'sin_month', 'cos_month', 'is_weekend']\n",
    "    max_store = df['store'].max()\n",
    "    max_item  = df['item'].max()\n",
    "\n",
    "    x_tr, y_tr, x_va, y_va = [], [], [], []\n",
    "\n",
    "    for (s, it), g in df.groupby(['store', 'item'], sort=True):\n",
    "        g = g.sort_values('date') \n",
    "        if len(g) < window_size + horizon:\n",
    "            continue\n",
    "\n",
    "        feat_mat = g[feats].to_numpy(dtype=np.float32)\n",
    "        store_norm = np.full((len(g), 1), s / max_store, dtype=np.float32)\n",
    "        item_norm  = np.full((len(g), 1), it / max_item,  dtype=np.float32)\n",
    "        x_full = np.concatenate([feat_mat, store_norm, item_norm], axis=1)\n",
    "        y_full = g['sales_log1p'].to_numpy(dtype=np.float32)\n",
    "        dates  = g['date'].to_numpy()\n",
    "\n",
    "        last_start = len(g) - (window_size + horizon)\n",
    "        start = 0\n",
    "        while start <= last_start:\n",
    "            end_hist   = start + window_size\n",
    "            end_target = end_hist + horizon\n",
    "            target_end_date = dates[end_target - 1]\n",
    "\n",
    "            x_win = x_full[start:end_hist, :]\n",
    "            y_win = y_full[end_hist:end_target]\n",
    "\n",
    "            if target_end_date >= val_start:\n",
    "                x_va.append(x_win); y_va.append(y_win)\n",
    "                start += stride_val\n",
    "            else:\n",
    "                x_tr.append(x_win); y_tr.append(y_win)\n",
    "                start += stride_train\n",
    "\n",
    "    x_tr = np.asarray(x_tr, dtype=np.float32)\n",
    "    y_tr = np.asarray(y_tr, dtype=np.float32)\n",
    "    x_va = np.asarray(x_va, dtype=np.float32)\n",
    "    y_va = np.asarray(y_va, dtype=np.float32)\n",
    "    return x_tr, y_tr, x_va, y_va\n",
    "\n",
    "# metricas de evaluacion inversa\n",
    "def smape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6) -> float:\n",
    "    yt = np.expm1(y_true)\n",
    "    yp = np.clip(np.expm1(y_pred), 0.0, None)\n",
    "    denom = (np.abs(yt) + np.abs(yp) + eps)\n",
    "    return float(np.mean(2.0 * np.abs(yp - yt) / denom))\n",
    "\n",
    "# error absoluto medio inverso\n",
    "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    yt = np.expm1(y_true)\n",
    "    yp = np.clip(np.expm1(y_pred), 0.0, None)\n",
    "    return float(np.mean(np.abs(yp - yt)))\n",
    "\n",
    "# ejecutar\n",
    "split_info = compute_time_split(train_feat, horizon=horizon)\n",
    "x_tr, y_tr, x_va, y_va = series_to_tensor_fast(\n",
    "    train_feat, window_size, horizon, split_info['val_start'],\n",
    "    stride_train=7, stride_val=7\n",
    ")\n",
    "\n",
    "# informacion de los tensores\n",
    "input_timesteps = x_tr.shape[1]\n",
    "input_features = x_tr.shape[2]\n",
    "print(\"x_tr:\", x_tr.shape, \"y_tr:\", y_tr.shape, \"| x_va:\", x_va.shape, \"y_va:\", y_va.shape)\n",
    "print(\"input shape (timesteps, features):\", (input_timesteps, input_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb338a25",
   "metadata": {},
   "source": [
    "## Selección de modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290e9ea",
   "metadata": {},
   "source": [
    "### justificacion\n",
    "\n",
    "Elegimos gru porque capta dependencias de largo plazo con menos parámetros que lstm, por lo que entrena y predice más rápido sin sacrificar precisión en ventanas largas. Frente a conv1d, el gru modela mejor estacionalidad y tendencias multidiarias, clave para un horizonte de 3 meses en series diarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4409d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "428/428 - 149s - 348ms/step - loss: 0.5102 - val_loss: 0.1371 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "428/428 - 135s - 316ms/step - loss: 0.1332 - val_loss: 0.1269 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "428/428 - 147s - 343ms/step - loss: 0.1309 - val_loss: 0.1245 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "428/428 - 141s - 328ms/step - loss: 0.1297 - val_loss: 0.1240 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "428/428 - 140s - 327ms/step - loss: 0.1290 - val_loss: 0.1226 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "428/428 - 138s - 322ms/step - loss: 0.1281 - val_loss: 0.1214 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "428/428 - 162s - 378ms/step - loss: 0.1276 - val_loss: 0.1208 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "428/428 - 141s - 329ms/step - loss: 0.1270 - val_loss: 0.1214 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "428/428 - 146s - 341ms/step - loss: 0.1262 - val_loss: 0.1191 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "428/428 - 132s - 307ms/step - loss: 0.1262 - val_loss: 0.1191 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\n",
      "validacion -> mae: 6.49684 | smape: 0.12149\n",
      "\n",
      "archivo de envio guardado en: submission.csv\n",
      "   id      sales\n",
      "0   0  15.148878\n",
      "1   1  16.037458\n",
      "2   2  17.072006\n",
      "3   3  18.683640\n",
      "4   4  19.586977\n",
      "5   5  13.029569\n",
      "6   6  15.094685\n",
      "7   7  15.424486\n",
      "8   8  16.401836\n",
      "9   9  17.356043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# seleccion de modelo GRU \n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# gpu + mixed precision si disponible\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "using_gpu = False\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "        using_gpu = True\n",
    "    except Exception as e:\n",
    "        print(\"mixed precision no disponible:\", e)\n",
    "\n",
    "# tf.data datasets\n",
    "def make_dataset(x, y, batch_size, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    if training:\n",
    "        ds = ds.shuffle(min(len(x), 10000), seed=42, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size, drop_remainder=False)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# crear datasets batch size adaptativo\n",
    "batch_size = 1024 if using_gpu else 256\n",
    "train_ds = make_dataset(x_tr, y_tr, batch_size, training=True)\n",
    "val_ds   = make_dataset(x_va, y_va, batch_size, training=False)\n",
    "\n",
    "# modelo gru \n",
    "def build_gru(input_timesteps: int, input_features: int, horizon: int) -> keras.Model:\n",
    "    inp = keras.Input(shape=(input_timesteps, input_features))\n",
    "    x = layers.GRU(64, return_sequences=True)(inp)\n",
    "    x = layers.GRU(32)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    out = layers.Dense(horizon, activation='linear', dtype='float32')(x)  \n",
    "    try:\n",
    "        model = keras.Model(inp, out)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mae', jit_compile=True)\n",
    "        return model\n",
    "    except Exception:\n",
    "        model = keras.Model(inp, out)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mae')\n",
    "        return model\n",
    "\n",
    "# construir modelo\n",
    "model = build_gru(input_timesteps, input_features, horizon)\n",
    "\n",
    "# callbacks para reducir lr y early stopping\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-5, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, verbose=1),\n",
    "]\n",
    "\n",
    "# entrenar por 10 epocas maximo, pero con early stopping\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# evaluacion en validacion\n",
    "y_pred_va = model.predict(x_va, batch_size=batch_size, verbose=0)\n",
    "val_mae = mae(y_va, y_pred_va)\n",
    "val_smape = smape(y_va, y_pred_va)\n",
    "print(f\"\\nvalidacion -> mae: {val_mae:.5f} | smape: {val_smape:.5f}\")\n",
    "\n",
    "# pronostico para el conjunto de prueba\n",
    "def build_last_windows_for_test(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    window_size: int\n",
    ") -> pd.DataFrame:\n",
    "    feats = ['sales_log1p', 'sin_dow', 'cos_dow', 'sin_month', 'cos_month', 'is_weekend']\n",
    "    max_store = df_train['store'].max()\n",
    "    max_item  = df_train['item'].max()\n",
    "    test_start_date = df_test['date'].min()\n",
    "\n",
    "    rows = []\n",
    "    for (s, it), g in df_train.groupby(['store', 'item'], sort=True):\n",
    "        g = g[g['date'] < test_start_date].sort_values('date') \n",
    "        if len(g) < window_size:\n",
    "            continue\n",
    "        g_last = g.iloc[-window_size:]\n",
    "        feat_mat = g_last[feats].to_numpy(dtype=np.float32)\n",
    "        store_norm = np.full((window_size, 1), s / max_store, dtype=np.float32)\n",
    "        item_norm  = np.full((window_size, 1), it / max_item,  dtype=np.float32)\n",
    "        x_win = np.concatenate([feat_mat, store_norm, item_norm], axis=1)\n",
    "        rows.append({'store': int(s), 'item': int(it), 'x_win': x_win})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# predecir para test\n",
    "def forecast_test(\n",
    "    model: keras.Model,\n",
    "    df_train_feat: pd.DataFrame,\n",
    "    df_test_feat: pd.DataFrame,\n",
    "    window_size: int,\n",
    "    horizon: int\n",
    ") -> pd.DataFrame:\n",
    "    windows_df = build_last_windows_for_test(df_train_feat, df_test_feat, window_size)\n",
    "    if windows_df.empty:\n",
    "        return pd.DataFrame(columns=['store', 'item', 'date', 'sales'])\n",
    "\n",
    "    future_dates = pd.date_range(\n",
    "        start=df_test_feat['date'].min(),\n",
    "        end=df_test_feat['date'].max(),\n",
    "        freq='D'\n",
    "    )\n",
    "    if len(future_dates) > horizon:\n",
    "        future_dates = future_dates[:horizon]\n",
    "    elif len(future_dates) < horizon:\n",
    "        future_dates = pd.date_range(start=future_dates.min(), periods=horizon, freq='D')\n",
    "\n",
    "    x_stack = np.stack(windows_df['x_win'].to_list(), axis=0).astype(np.float32)\n",
    "    y_hat = model.predict(x_stack, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    preds_rows = []\n",
    "    for idx, row in windows_df.iterrows():\n",
    "        s = row['store']; it = row['item']\n",
    "        yh = np.expm1(y_hat[idx]); yh = np.clip(yh, 0.0, None)\n",
    "        preds_rows.append(pd.DataFrame({'store': s, 'item': it, 'date': future_dates, 'sales': yh}))\n",
    "    return pd.concat(preds_rows, ignore_index=True)\n",
    "\n",
    "preds_daily = forecast_test(\n",
    "    model,\n",
    "    df_train_feat=train_feat,\n",
    "    df_test_feat=test_feat,\n",
    "    window_size=window_size,\n",
    "    horizon=horizon\n",
    ")\n",
    "\n",
    "# construir archivo de envio\n",
    "def build_submission(preds_daily: pd.DataFrame, test_df: pd.DataFrame, out_path: str) -> pd.DataFrame:\n",
    "    out = test_df.merge(preds_daily, on=['store', 'item', 'date'], how='left').copy()\n",
    "    out['sales'] = out['sales'].fillna(0.0)\n",
    "    if 'id' in out.columns:\n",
    "        submit = out[['id', 'sales']].copy()\n",
    "    else:\n",
    "        submit = out[['store', 'item', 'date', 'sales']].copy()\n",
    "    submit.to_csv(out_path, index=False)\n",
    "    return submit\n",
    "\n",
    "# archivo de envio\n",
    "submission_path = \"submission.csv\"\n",
    "submission_df = build_submission(preds_daily, test_raw, submission_path)\n",
    "print(\"\\narchivo de envio guardado en:\", submission_path)\n",
    "print(submission_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfef71",
   "metadata": {},
   "source": [
    "## Output Submission\n",
    "### 45,000 filas 10 tiendas x 50 articulos = 500 series que cubren 3 meses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
